

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Performance Comparison of Dimension Reduction Implementations &mdash; umap 0.3 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/gallery.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="UMAP API Guide" href="api.html" />
    <link rel="prev" title="How UMAP Works" href="how_umap_works.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> umap
          

          
          </a>

          
            
            
              <div class="version">
                0.3
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">User Guide / Tutorial:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="basic_usage.html">How to Use UMAP</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameters.html">Basic UMAP Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="transform.html">Transforming New Data with UMAP</a></li>
<li class="toctree-l1"><a class="reference internal" href="supervised.html">UMAP for Supervised Dimension Reduction and Metric Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="clustering.html">Using UMAP for Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_examples/index.html">Gallery of Examples of UMAP usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">Frequently Asked Questions</a></li>
</ul>
<p class="caption"><span class="caption-text">Background on UMAP:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="how_umap_works.html">How UMAP Works</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Performance Comparison of Dimension Reduction Implementations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#performance-scaling-by-dataset-size">Performance scaling by dataset size</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API Reference:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api.html">UMAP API Guide</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">umap</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Performance Comparison of Dimension Reduction Implementations</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/benchmarking.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="performance-comparison-of-dimension-reduction-implementations">
<h1>Performance Comparison of Dimension Reduction Implementations<a class="headerlink" href="#performance-comparison-of-dimension-reduction-implementations" title="Permalink to this headline">¶</a></h1>
<p>Different dimension reduction techniques can have quite different
computational complexity. Beyond the algorithm itself there is also the
question of how exactly it is implemented. These two factors can have a
significant role in how long it actually takes to run a given dimension
reduction. Furthermore the nature of the data you are trying to reduce
can also matter – mostly the involves the dimensionality of the
original data. Here we will take a brief look at the performance
characterstics of a number of dimension reduction implementations.</p>
<p>To start let’s get the basic tools we’ll need loaded up – numpy and
pandas obviously, but also tools to get and resample the data, and the
time module so we can perform some basic benchmarking.</p>
<div class="code ipython3 highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">fetch_mldata</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="k">import</span> <span class="n">resample</span>
<span class="kn">import</span> <span class="nn">time</span>
</pre></div>
</div>
<p>Next we’ll need the actual dimension reduction implementations. For the
purposes of this explanation we’ll mostly stick with
<a class="reference external" href="http://scikit-learn.org/stable/">scikit-learn</a>, but for the sake of
comparison we’ll also include the
<a class="reference external" href="https://github.com/DmitryUlyanov/Multicore-TSNE">MulticoreTSNE</a>
implementation of t-SNE, which has significantly better performance than
the current scikit-learn t-SNE.</p>
<div class="code ipython3 highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="k">import</span> <span class="n">TSNE</span><span class="p">,</span> <span class="n">LocallyLinearEmbedding</span><span class="p">,</span> <span class="n">Isomap</span><span class="p">,</span> <span class="n">MDS</span><span class="p">,</span> <span class="n">SpectralEmbedding</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="k">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">MulticoreTSNE</span> <span class="k">import</span> <span class="n">MulticoreTSNE</span>
<span class="kn">from</span> <span class="nn">umap</span> <span class="k">import</span> <span class="n">UMAP</span>
</pre></div>
</div>
<p>Next we’ll need out plotting tools, and, of course, some data to work
with. For this performance comparison we’ll default to the now standard
benchmark of manifold learning: the MNIST digits dataset. We can use
scikit-learn’s <code class="docutils literal notranslate"><span class="pre">fetch_mldata</span></code> to grab it for us.</p>
<div class="code ipython3 highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</pre></div>
</div>
<div class="code ipython3 highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">context</span><span class="o">=</span><span class="s1">&#39;notebook&#39;</span><span class="p">,</span>
        <span class="n">rc</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">:(</span><span class="mi">12</span><span class="p">,</span><span class="mi">10</span><span class="p">)},</span>
        <span class="n">palette</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s1">&#39;tab10&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
<div class="code ipython3 highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mnist</span> <span class="o">=</span> <span class="n">fetch_mldata</span><span class="p">(</span><span class="s1">&#39;MNIST Original&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Now it is time to start looking at performance. To start with let’s look
at how performance scales with increasing dataset size.</p>
<div class="section" id="performance-scaling-by-dataset-size">
<h2>Performance scaling by dataset size<a class="headerlink" href="#performance-scaling-by-dataset-size" title="Permalink to this headline">¶</a></h2>
<p>As the size of a dataset increases the runtime of a given dimension
reduction algorithm will increase at varying rates. If you ever want to
run your algorithm on larger datasets you will care not just about the
comparative runtime on a single small dataset, but how the performance
scales out as you move to larger datasets. We can similate this by
subsampling from MNIST digits (via scikit-learn’s convenient
<code class="docutils literal notranslate"><span class="pre">resample</span></code> utility) and looking at the runtime for varying sized
subsamples. Since there is some randomness involved here (both in the
subsample selection, and in some of the algorithms which have stochastic
aspects) we will want to run a few examples for each dataset size. We
can easily package all of this up in a simple function that will return
a convenient pandas dataframe of dataset sizes and runtimes given an
algorithm.</p>
<div class="code ipython3 highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">data_size_scaling</span><span class="p">(</span><span class="n">algorithm</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">800</span><span class="p">,</span> <span class="mi">1600</span><span class="p">],</span> <span class="n">n_runs</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">sizes</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">run</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_runs</span><span class="p">):</span>
            <span class="n">subsample</span> <span class="o">=</span> <span class="n">resample</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>
            <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
            <span class="n">algorithm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">subsample</span><span class="p">)</span>
            <span class="n">elapsed_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
            <span class="k">del</span> <span class="n">subsample</span>
            <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">size</span><span class="p">,</span> <span class="n">elapsed_time</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;dataset size&#39;</span><span class="p">,</span> <span class="s1">&#39;runtime (s)&#39;</span><span class="p">))</span>
</pre></div>
</div>
<p>Now we just want to run this for each of the various dimension reduction
implementations so we can look at the results. Since we don’t know how
long these runs might take we’ll start off with a very small set of
samples, scaling up to only 1600 samples.</p>
<div class="code ipython3 highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">all_algorithms</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">PCA</span><span class="p">(),</span>
    <span class="n">UMAP</span><span class="p">(),</span>
    <span class="n">MulticoreTSNE</span><span class="p">(),</span>
    <span class="n">LocallyLinearEmbedding</span><span class="p">(),</span>
    <span class="n">SpectralEmbedding</span><span class="p">(),</span>
    <span class="n">Isomap</span><span class="p">(),</span>
    <span class="n">TSNE</span><span class="p">(),</span>
    <span class="n">MDS</span><span class="p">(),</span>
<span class="p">]</span>
<span class="n">performance_data</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">algorithm</span> <span class="ow">in</span> <span class="n">all_algorithms</span><span class="p">:</span>
    <span class="n">alg_name</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">algorithm</span><span class="p">)</span>
    <span class="k">if</span> <span class="s1">&#39;MulticoreTSNE&#39;</span> <span class="ow">in</span> <span class="n">alg_name</span><span class="p">:</span>
        <span class="n">alg_name</span> <span class="o">=</span> <span class="s1">&#39;MulticoreTSNE&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">alg_name</span> <span class="o">=</span> <span class="n">alg_name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;(&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">performance_data</span><span class="p">[</span><span class="n">alg_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">data_size_scaling</span><span class="p">(</span><span class="n">algorithm</span><span class="p">,</span> <span class="n">mnist</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">n_runs</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<p>Now let’s plot the results so we can see what is going on. We’ll use
seaborn’s regression plot to interpolate the effective scaling.</p>
<div class="code ipython3 highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">alg_name</span><span class="p">,</span> <span class="n">perf_data</span> <span class="ow">in</span> <span class="n">performance_data</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span><span class="s1">&#39;dataset size&#39;</span><span class="p">,</span> <span class="s1">&#39;runtime (s)&#39;</span><span class="p">,</span> <span class="n">perf_data</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">alg_name</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
<img alt="_images/performance_14_1.png" src="_images/performance_14_1.png" />
<p>We can see straight away that there are some outliers here. The
scikit-learn t-SNE is clearly much slower than most of the other
algorithms. It does not have the scaling properties of MDS however; for
larger dataset sizes MDS is going to quickly become completely
unmanageable. At the same time MulticoreTSNE demonstrates that t-SNE can
run fairly efficiently. It is hard to tell much about the other
implementations other than the fact that PCA is far and away the fastest
option. To see more we’ll have to look at runtimes on larger dataset
sizes. Both MDS and scikit-learn’s t-SNE are going to take too long to
run so let’s restrict ourselves to the fastest performing
implementations and see what happens as we extend out to larger dataset
sizes.</p>
<div class="code ipython3 highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">fast_algorithms</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">PCA</span><span class="p">(),</span>
    <span class="n">UMAP</span><span class="p">(),</span>
    <span class="n">MulticoreTSNE</span><span class="p">(),</span>
    <span class="n">LocallyLinearEmbedding</span><span class="p">(),</span>
    <span class="n">SpectralEmbedding</span><span class="p">(),</span>
    <span class="n">Isomap</span><span class="p">(),</span>
<span class="p">]</span>
<span class="n">fast_performance_data</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">algorithm</span> <span class="ow">in</span> <span class="n">fast_algorithms</span><span class="p">:</span>
    <span class="n">alg_name</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">algorithm</span><span class="p">)</span>
    <span class="k">if</span> <span class="s1">&#39;MulticoreTSNE&#39;</span> <span class="ow">in</span> <span class="n">alg_name</span><span class="p">:</span>
        <span class="n">alg_name</span> <span class="o">=</span> <span class="s1">&#39;MulticoreTSNE&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">alg_name</span> <span class="o">=</span> <span class="n">alg_name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;(&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">fast_performance_data</span><span class="p">[</span><span class="n">alg_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">data_size_scaling</span><span class="p">(</span><span class="n">algorithm</span><span class="p">,</span> <span class="n">mnist</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
                                                   <span class="n">sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">800</span><span class="p">,</span> <span class="mi">1600</span><span class="p">,</span> <span class="mi">3200</span><span class="p">,</span> <span class="mi">6400</span><span class="p">,</span> <span class="mi">12800</span><span class="p">],</span> <span class="n">n_runs</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<div class="code ipython3 highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">alg_name</span><span class="p">,</span> <span class="n">perf_data</span> <span class="ow">in</span> <span class="n">fast_performance_data</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span><span class="s1">&#39;dataset size&#39;</span><span class="p">,</span> <span class="s1">&#39;runtime (s)&#39;</span><span class="p">,</span> <span class="n">perf_data</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">alg_name</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
<img alt="_images/performance_17_1.png" src="_images/performance_17_1.png" />
<p>At this point we begin to see some significant differentiation among the
different implementations. In the earlier plot MulticoreTSNE looked to
be slower than some of the other algorithms, but as we scale out to
larger datasets we see that its relative scaling performance is far
superior to the scikit-learn implementations of Isomap, spectral
embedding, and locally linear embedding.</p>
<p>It is probably worth extending out further – up to the full MNIST
digits dataset. To manage to do that in any reasonable amount of time
we’ll have to restrict out attention to an even smaller subset of
implementations. We will pare things down to just MulticoreTSNE, PCA and
UMAP.</p>
<div class="code ipython3 highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">very_fast_algorithms</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">PCA</span><span class="p">(),</span>
    <span class="n">UMAP</span><span class="p">(),</span>
    <span class="n">MulticoreTSNE</span><span class="p">(),</span>
<span class="p">]</span>
<span class="n">vfast_performance_data</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">algorithm</span> <span class="ow">in</span> <span class="n">very_fast_algorithms</span><span class="p">:</span>
    <span class="n">alg_name</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">algorithm</span><span class="p">)</span>
    <span class="k">if</span> <span class="s1">&#39;MulticoreTSNE&#39;</span> <span class="ow">in</span> <span class="n">alg_name</span><span class="p">:</span>
        <span class="n">alg_name</span> <span class="o">=</span> <span class="s1">&#39;MulticoreTSNE&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">alg_name</span> <span class="o">=</span> <span class="n">alg_name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;(&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">vfast_performance_data</span><span class="p">[</span><span class="n">alg_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">data_size_scaling</span><span class="p">(</span><span class="n">algorithm</span><span class="p">,</span> <span class="n">mnist</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
                                                    <span class="n">sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">3200</span><span class="p">,</span> <span class="mi">6400</span><span class="p">,</span> <span class="mi">12800</span><span class="p">,</span> <span class="mi">25600</span><span class="p">,</span> <span class="mi">51200</span><span class="p">,</span> <span class="mi">70000</span><span class="p">],</span> <span class="n">n_runs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<div class="code ipython3 highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">alg_name</span><span class="p">,</span> <span class="n">perf_data</span> <span class="ow">in</span> <span class="n">vfast_performance_data</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span><span class="s1">&#39;dataset size&#39;</span><span class="p">,</span> <span class="s1">&#39;runtime (s)&#39;</span><span class="p">,</span> <span class="n">perf_data</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">alg_name</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
<img alt="_images/performance_20_1.png" src="_images/performance_20_1.png" />
<p>Here we see UMAP’s advantages over t-SNE really coming to the forefront.
While UMAP is clearly slower than PCA, its scaling performance is
dramatically better than MulticoreTSNE, and for even larger datasets the
difference is only going to grow.</p>
<p>This concludes our look at scaling by dataset size. The short summary is
that PCA is far and away the fastest option, but you are potentially
giving up a lot for that speed. UMAP, while not competitive with PCA, is
clearly the next best option in terms of performance among the
implementations explored here. Given the quality of results that UMAP
can provide we feel it is clearly a good option for dimension reduction.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="api.html" class="btn btn-neutral float-right" title="UMAP API Guide" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="how_umap_works.html" class="btn btn-neutral" title="How UMAP Works" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Leland McInnes.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.3',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>